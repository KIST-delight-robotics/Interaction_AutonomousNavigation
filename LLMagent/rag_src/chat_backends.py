from __future__ import annotations
## only about chat tools

# chat_backends.py
"""
ì—°êµ¬ì†Œ ê´€ë ¨ ëŒ€í™”(RAG)ì™€ ì¼ë°˜ ëŒ€í™”(ê³ ì„±ëŠ¥ LLM)ë¥¼ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆ.
agent_llm.pyì˜ @tool í•¨ìˆ˜ë“¤ì€ ì—¬ê¸° í•¨ìˆ˜ë“¤ì„ í˜¸ì¶œë§Œ í•œë‹¤.
"""
import json
from pathlib import Path
from operator import itemgetter
from typing import Optional,List
from datetime import datetime, timezone

from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_community.document_loaders import DirectoryLoader,TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

from langchain.chat_models import init_chat_model

# ğŸ”¹ ì¼ë°˜ ëŒ€í™”ìš©
general_llm = ChatOllama(model="exaone3.5:7.8b")


# ===== ì „ì—­ ìºì‹œ =====
_RAG_VECTORSTORE = None
_RAG_RETRIEVER = None
_RAG_CHAIN = None

BASE_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = BASE_DIR.parent                      # .../your_project

RAG_DIR = PROJECT_ROOT / "rag_file"
RAG_DB_DIR = PROJECT_ROOT / "_rag_chroma_db"

# RAG ì„¤ì •
EMBED_MODEL = "intfloat/multilingual-e5-base"
CHUNK_SIZE = 700
CHUNK_OVERLAP = 100
TOP_K = 4

####################
# í•¨ìˆ˜ tools
####################

def get_vectorstore() -> Chroma:
    global _RAG_VECTORSTORE
    if _RAG_VECTORSTORE is not None:
        return _RAG_VECTORSTORE

    # 1) ë¡œë“œ ì‹œë„
    vs = _load_vectorstore_if_exists()
    if vs is None:
        # 2) ì—†ìœ¼ë©´ ë¹Œë“œ
        vs = build_rag_vectorstore()

    _RAG_VECTORSTORE = vs
    return _RAG_VECTORSTORE

def get_retriever():
    global _RAG_RETRIEVER
    if _RAG_RETRIEVER is None:
        _RAG_RETRIEVER = get_vectorstore().as_retriever(search_kwargs={"k": TOP_K})
    return _RAG_RETRIEVER

def get_chain():
    global _RAG_CHAIN
    if _RAG_CHAIN is None:
        _RAG_CHAIN = _build_lab_rag_chain(get_retriever())
    return _RAG_CHAIN

def reset_rag_cache(reset_vectorstore: bool = False):
    """
    ì‹¤í—˜ ì¤‘ íŒŒë¼ë¯¸í„°(k/í”„ë¡¬í”„íŠ¸/ì²´ì¸ ë“±) ë°”ê¿€ ë•Œ ìºì‹œ ì´ˆê¸°í™”.
    vectorstoreê¹Œì§€ ì´ˆê¸°í™”í•˜ë©´ ë‹¤ìŒ í˜¸ì¶œì—ì„œ ë¡œë“œ/ë¹Œë“œë¥¼ ë‹¤ì‹œ í•¨.
    """
    global _RAG_VECTORSTORE, _RAG_RETRIEVER, _RAG_CHAIN
    _RAG_CHAIN = None
    _RAG_RETRIEVER = None
    if reset_vectorstore:
        _RAG_VECTORSTORE = None

def _retrieve_docs(retriever, query: str) -> List[Document]:
    """
    LangChain ë²„ì „ ì°¨ì´ë¥¼ í¡ìˆ˜í•˜ëŠ” ì•ˆì „ retriever í˜¸ì¶œ.
    ìµœì‹ : retriever.invoke(query)
    êµ¬ë²„ì „: retriever.get_relevant_documents(query)
    ìµœí›„: retriever._get_relevant_documents(query) (private)
    """
    if hasattr(retriever, "invoke"):
        return retriever.invoke(query)
    if hasattr(retriever, "get_relevant_documents"):
        return retriever.get_relevant_documents(query)
    if hasattr(retriever, "_get_relevant_documents"):
        return retriever._get_relevant_documents(query)
    raise AttributeError("Retriever has no supported retrieval method (invoke/get_relevant_documents/_get_relevant_documents).")


# retrieverê°€ ëŒë ¤ì¤€ document  ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜!! (context formatí•¨ìˆ˜)
def _format_docs(docs: List[Document]) -> str:
    """retriever ê²°ê³¼ Document ë¦¬ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ í¬ë§·."""
    if not docs:
        return "ê´€ë ¨ ì—°êµ¬ì†Œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    parts = []
    for i, d in enumerate(docs, start=1):
        meta = d.metadata or {}
        src = meta.get("source", "")
        source_type = meta.get("source_type", "")

        header_bits = [f"[{i}]"]
        if source_type:
            header_bits.append(f"({source_type})")
        if src:
            header_bits.append(f"- {src}")

        header = " ".join(header_bits)
        parts.append(f"{header}\n{d.page_content}")

    return "\n\n".join(parts)


# ---------- 1) Indexing: ---------- # 
#-------------------------- loader ---------------------- #
def load_rag_corpus(rag_dir: str | Path) -> List[Document]:
    """
    rag_file í´ë” ì•ˆì˜ corpus.json + .md íŒŒì¼ë“¤ì„ ëª¨ë‘ ì½ì–´ì„œ
    LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” loader.
    
    """
    rag_path = Path(rag_dir)

    if not rag_path.exists():
        raise FileNotFoundError(f"[RAG_LOADER] rag_dir not found: {rag_path}")

    all_docs: List[Document] = []

    # 1) Markdown íŒŒì¼ë“¤ ë¡œë“œ (unstructured ë§ê³  ê·¸ëƒ¥ TextLoader ì‚¬ìš©)
    md_loader = DirectoryLoader(
        str(rag_path),
        glob="**/*.md",
        show_progress=True,
        loader_cls=TextLoader,              # â˜… í•µì‹¬: TextLoaderë¡œ ê°•ì œ
        loader_kwargs={"encoding": "utf-8"}
    )
    md_docs = md_loader.load()
    for d in md_docs:
        d.metadata = d.metadata or {}
        d.metadata.setdefault("source_type", "markdown")
        all_docs.append(d)

    # 2) corpus.json ë¡œë“œ
    json_path = rag_path / "corpus.json"
    if json_path.exists():
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # corpus.jsonì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ê³  ê°€ì •
        if not isinstance(data, list):
            print("[RAG_LOADER] corpus.json is not a list. Adjust parsing logic if needed.")
            data = [data]

        json_docs: List[Document] = []
        for idx, item in enumerate(data):
            if not isinstance(item, dict):
                # í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¥´ë©´ ìŠ¤í‚µ
                continue

            # text/content/body ì¤‘ì—ì„œ ì‹¤ì œ ë³¸ë¬¸ì— í•´ë‹¹í•˜ëŠ” í‚¤ë¥¼ ì°¾ì•„ë³¸ë‹¤.
            text = (
                item.get("text")
                or item.get("content")
                or item.get("body")
                or ""
            )
            if not text.strip():
                continue

            # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” ë©”íƒ€ë°ì´í„°ë¡œ ì‚¬ìš©
            metadata = {
                k: v
                for k, v in item.items()
                if k not in ("text", "content", "body")
            }
            metadata["source_type"] = "corpus_json"
            metadata["source"] = str(json_path)
            metadata.setdefault("index", idx)

            json_docs.append(
                Document(
                    page_content=text,
                    metadata=metadata,
                )
            )

        print(f"[RAG_LOADER] corpus.json docs: {len(json_docs)}")
        all_docs.extend(json_docs)
    else:
        print(f"[RAG_LOADER] corpus.json not found in {rag_path}, only md files will be used.")

    print(f"[RAG_LOADER] total docs: {len(all_docs)}")
    return all_docs

#-------------------------- vectorstore  ---------------------- #

def build_rag_vectorstore() -> Chroma:
    # 1) JSON + MD ëª¨ë‘ ë¡œë“œ
    docs = load_rag_corpus(RAG_DIR)

    # 2) ì²­í¬ ë‚˜ëˆ„ê¸°
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
    )
    splits = splitter.split_documents(docs)

    # 3) ì„ë² ë”© + vector store
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL
    )

    RAG_DB_DIR.mkdir(parents=True, exist_ok=True)
    vectorstore = Chroma.from_documents(
        splits,
        embedding=embeddings,
        persist_directory=str(RAG_DB_DIR),
    )
    vectorstore.persist()

    print("[LAB_RAG] vector store build ì™„ë£Œ")
    return vectorstore


def _load_vectorstore_if_exists() -> Optional[Chroma]:
    """
    persist ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ë¡œë“œë§Œ í•œë‹¤.
    (ì£¼ì˜: ì´ ê²½ë¡œì— ìœ íš¨í•œ Chroma DBê°€ ìˆì–´ì•¼ í•¨)
    """
    if not RAG_DB_DIR.exists():
        return None

    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
    vectorstore = Chroma(
        persist_directory=str(RAG_DB_DIR),
        embedding_function=embeddings,
    )
    print("[LAB_RAG] vector store LOAD ì™„ë£Œ")
    return vectorstore




# ---------- 2) Retriever & RAG ì²´ì¸ êµ¬ì„± ----------

def _build_lab_rag_chain(retriever):
    """
    question(str) -> answer(str) í˜•íƒœë¡œ ë™ì‘í•˜ëŠ” RAG ì²´ì¸ ìƒì„±.
    - retriever: rag_file ê¸°ë°˜ (corpus.json + md)
    - prompt: 'ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œë§Œ ëŒ€ë‹µí•´ë¼'
    """
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                (
                    "ë„ˆëŠ” KIST ë° ì—°êµ¬ì‹¤ ì•ˆë‚´ assistantì´ë‹¤. "
                    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë¬¸ì¥ì„  `question`ì¸ì ê·¸ëŒ€ë¡œ ë„£ì–´ë¼"
                    "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•´ë¼. "
                    "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´ë¼.\n\n"
                    "ì»¨í…ìŠ¤íŠ¸:\n{context}"
                ),
            ),
            ("human", "ì§ˆë¬¸: {question}"),
        ]
    )

    llm = general_llm

    _RAG_CHAIN  = (
        {
            "context": itemgetter("question") | retriever,
            "question": itemgetter("question"),
        }
        | RunnableLambda(
            lambda x: {
                "context": _format_docs(x["context"]),
                "question": x["question"],
            }
        )
        | prompt
        | llm
        | StrOutputParser()
    )

    print("[LAB_RAG] rag_chain ìƒì„± ì™„ë£Œ")
    return _RAG_CHAIN 

# ---------- 3) Entry points ----------

def run_lab_rag(question: str) -> str:
    """
    KIST/ì—°êµ¬ì‹¤ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ RAG ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.

    - agent_llm.py ì˜ lab_chat íˆ´ì—ì„œ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•œë‹¤.
    - ë‚´ë¶€ì ìœ¼ë¡œ rag_chain (retriever + prompt + llm)ì„ ì‚¬ìš©.
    """

    print(f"[LAB_RAG] run_lab_rag í˜¸ì¶œ. question={question!r}")
    try:
        answer = get_chain().invoke({"question": question})
        print("[LAB_RAG] run_lab_rag.invoke ì™„ë£Œ")
        return answer
    except Exception as e:
        print(f"[LAB_RAG] ì˜¤ë¥˜: {e}")
        return "ì—°êµ¬ì†Œ ê´€ë ¨ RAG ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def run_lab_rag_with_trace(question: str) -> tuple[str, List[dict[str, any]]]:
    """
    í‰ê°€/ë¡œê·¸ ëª©ì : answer + retrieved docs(contexts)ê¹Œì§€ ë°˜í™˜
    """
    retriever = get_retriever()

    # 1) retrieve (ë²„ì „ í˜¸í™˜)
    docs = _retrieve_docs(retriever, question)

    # 2) generate
    answer = get_chain().invoke({"question": question})
    # 3) contexts serialize-friendly ë³€í™˜
    contexts = []
    for d in docs:
        contexts.append({
            "text": d.page_content,
            "metadata": d.metadata or {}
        })

    return answer, contexts

def run_general_chat(question: str) -> str:
    """
    ì—°êµ¬ì†Œì™€ ë¬´ê´€í•œ ì¼ë°˜ ëŒ€í™”ë¥¼ ìœ„í•œ LLM í˜¸ì¶œ í•¨ìˆ˜.
    """

    resp = general_llm.invoke(question)

    # ChatOllama ì‘ë‹µ í¬ë§·ì— ë”°ë¼ content êº¼ë‚´ê¸°
    if hasattr(resp, "content"):
        return resp.content
    return str(resp)




# if __name__ == "__main__":
#     # ê°„ë‹¨í•œ ë””ë²„ê·¸ìš© ì§ˆë¬¸ë“¤
#     test_questions = [
#         "kistëŠ” ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‹ˆ?",
#         "ì´ ì—°êµ¬ì‹¤ì€ ì–´ë–¤ ì—°êµ¬ë¥¼ í•´?",
#         "ì¥ê¸°í˜„ì¥ì‹¤ìŠµ ë¶„ìœ„ê¸° ì–´ë• ì–´?",
#     ]

#     from pprint import pprint

#     for q in test_questions:
#         print("\n" + "=" * 80)
#         print(f"[TEST] ì§ˆë¬¸: {q}")
#         print("=" * 80)

#         answer = run_lab_rag(q)
#         print("[TEST] ë‹µë³€:")
#         print(answer)